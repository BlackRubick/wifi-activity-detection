"""
M√≥dulo de modelos de machine learning para detecci√≥n de actividades
==================================================================

Este m√≥dulo implementa diferentes modelos para reconocimiento de actividades:
- Modelos tradicionales (SVM, Random Forest, XGBoost)
- Redes neuronales profundas (CNN 1D, CNN 2D, LSTM)
- Modelos h√≠bridos (CNN + LSTM)
- Optimizaci√≥n de hiperpar√°metros
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Conv1D, Conv2D, LSTM, GRU, Dropout, BatchNormalization
from tensorflow.keras.layers import MaxPooling1D, MaxPooling2D, Flatten, Input, concatenate
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.utils import to_categorical
import xgboost as xgb
import warnings

warnings.filterwarnings('ignore')


class ActivityRecognitionModels:
    """
    Clase para entrenar y evaluar modelos de reconocimiento de actividades
    """

    def __init__(self, random_state=42):
        """
        Inicializa el sistema de modelos

        Args:
            random_state (int): Semilla para reproducibilidad
        """
        self.random_state = random_state
        self.models = {}
        self.history = {}
        self.results = {}

        # Configurar seeds para reproducibilidad
        np.random.seed(random_state)
        tf.random.set_seed(random_state)

    def prepare_data(self, features, labels, test_size=0.3, validation_size=0.2):
        """
        Prepara los datos para entrenamiento - VERSI√ìN PARA DATASETS PEQUE√ëOS
        """
        print("Preparando datos para entrenamiento...")

        # Codificar etiquetas
        self.label_encoder = LabelEncoder()
        labels_encoded = self.label_encoder.fit_transform(labels)
        self.n_classes = len(np.unique(labels_encoded))
        self.class_names = self.label_encoder.classes_

        print(f"Dataset: {len(features)} muestras, {self.n_classes} clases")
        print(f"Distribuci√≥n: {np.bincount(labels_encoded)}")

        # AJUSTAR PARA DATASETS PEQUE√ëOS
        n_samples = len(features)
        if n_samples < 50:
            test_size = 0.25
            validation_size = 0.15 if n_samples > 20 else 0.0
            print(f"‚ö†Ô∏è Dataset peque√±o - Ajustando divisi√≥n: test={test_size}, val={validation_size}")

        # Divisi√≥n sin estratificaci√≥n si hay muy pocas muestras
        use_stratify = n_samples >= self.n_classes * 4
        stratify = labels_encoded if use_stratify else None

        # Divisi√≥n train/test
        if test_size > 0 and n_samples > 4:
            try:
                self.X_train_temp, self.X_test, self.y_train_temp, self.y_test = train_test_split(
                    features, labels_encoded, test_size=test_size,
                    random_state=self.random_state, stratify=stratify
                )
            except ValueError:
                # Sin estratificaci√≥n si falla
                self.X_train_temp, self.X_test, self.y_train_temp, self.y_test = train_test_split(
                    features, labels_encoded, test_size=test_size,
                    random_state=self.random_state
                )
        else:
            # Muy pocas muestras - usar todo para entrenamiento
            self.X_train_temp, self.y_train_temp = features, labels_encoded
            self.X_test, self.y_test = features[:2], labels_encoded[:2]  # Muestras m√≠nimas para test

        # Divisi√≥n train/validation
        if validation_size > 0 and len(self.X_train_temp) > 5:
            try:
                val_size = validation_size / (1 - test_size)
                self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(
                    self.X_train_temp, self.y_train_temp, test_size=val_size,
                    random_state=self.random_state
                )
            except:
                # Si falla, usar train como validaci√≥n
                self.X_train, self.y_train = self.X_train_temp, self.y_train_temp
                self.X_val, self.y_val = self.X_train, self.y_train
        else:
            # Sin validaci√≥n separada
            self.X_train, self.y_train = self.X_train_temp, self.y_train_temp
            self.X_val, self.y_val = self.X_train, self.y_train

        # Normalizar caracter√≠sticas
        self.scaler = StandardScaler()
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_val_scaled = self.scaler.transform(self.X_val)
        self.X_test_scaled = self.scaler.transform(self.X_test)

        # Preparar etiquetas categ√≥ricas para deep learning
        self.y_train_cat = to_categorical(self.y_train, num_classes=self.n_classes)
        self.y_val_cat = to_categorical(self.y_val, num_classes=self.n_classes)
        self.y_test_cat = to_categorical(self.y_test, num_classes=self.n_classes)

        print(f"Divisi√≥n final:")
        print(f"  Train: {self.X_train.shape[0]} muestras")
        print(f"  Val: {self.X_val.shape[0]} muestras")
        print(f"  Test: {self.X_test.shape[0]} muestras")
        print(f"  Caracter√≠sticas: {self.X_train.shape[1]}")

        return self.X_train_scaled, self.X_val_scaled, self.X_test_scaled
    def train_random_forest(self, n_estimators=100, max_depth=None, **kwargs):
        """
        Entrena modelo Random Forest

        Args:
            n_estimators (int): N√∫mero de √°rboles
            max_depth (int): Profundidad m√°xima
            **kwargs: Par√°metros adicionales

        Returns:
            RandomForestClassifier: Modelo entrenado
        """
        print("Entrenando Random Forest...")

        rf_model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=self.random_state,
            **kwargs
        )

        # Entrenar
        rf_model.fit(self.X_train_scaled, self.y_train)

        # Evaluar
        train_acc = rf_model.score(self.X_train_scaled, self.y_train)
        val_acc = rf_model.score(self.X_val_scaled, self.y_val)
        test_acc = rf_model.score(self.X_test_scaled, self.y_test)

        # Predicciones
        y_pred_train = rf_model.predict(self.X_train_scaled)
        y_pred_val = rf_model.predict(self.X_val_scaled)
        y_pred_test = rf_model.predict(self.X_test_scaled)

        self.models['random_forest'] = rf_model
        self.results['random_forest'] = {
            'train_accuracy': train_acc,
            'val_accuracy': val_acc,
            'test_accuracy': test_acc,
            'predictions_test': y_pred_test,
            'feature_importance': rf_model.feature_importances_
        }

        print(f"Random Forest - Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}")
        return rf_model

    def train_svm(self, C=1.0, kernel='rbf', **kwargs):
        """
        Entrena modelo SVM

        Args:
            C (float): Par√°metro de regularizaci√≥n
            kernel (str): Tipo de kernel
            **kwargs: Par√°metros adicionales

        Returns:
            SVC: Modelo entrenado
        """
        print("Entrenando SVM...")

        svm_model = SVC(
            C=C,
            kernel=kernel,
            random_state=self.random_state,
            probability=True,  # Para obtener probabilidades
            **kwargs
        )

        # Entrenar
        svm_model.fit(self.X_train_scaled, self.y_train)

        # Evaluar
        train_acc = svm_model.score(self.X_train_scaled, self.y_train)
        val_acc = svm_model.score(self.X_val_scaled, self.y_val)
        test_acc = svm_model.score(self.X_test_scaled, self.y_test)

        # Predicciones
        y_pred_test = svm_model.predict(self.X_test_scaled)
        y_pred_proba = svm_model.predict_proba(self.X_test_scaled)

        self.models['svm'] = svm_model
        self.results['svm'] = {
            'train_accuracy': train_acc,
            'val_accuracy': val_acc,
            'test_accuracy': test_acc,
            'predictions_test': y_pred_test,
            'predictions_proba': y_pred_proba
        }

        print(f"SVM - Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}")
        return svm_model

    def train_xgboost(self, n_estimators=100, max_depth=6, learning_rate=0.1, **kwargs):
        """
        Entrena modelo XGBoost

        Args:
            n_estimators (int): N√∫mero de estimadores
            max_depth (int): Profundidad m√°xima
            learning_rate (float): Tasa de aprendizaje
            **kwargs: Par√°metros adicionales

        Returns:
            XGBClassifier: Modelo entrenado
        """
        print("Entrenando XGBoost...")

        xgb_model = xgb.XGBClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            learning_rate=learning_rate,
            random_state=self.random_state,
            eval_metric='mlogloss',
            **kwargs
        )

        # Entrenar
        xgb_model.fit(self.X_train_scaled, self.y_train)

        # Evaluar
        train_acc = xgb_model.score(self.X_train_scaled, self.y_train)
        val_acc = xgb_model.score(self.X_val_scaled, self.y_val)
        test_acc = xgb_model.score(self.X_test_scaled, self.y_test)

        # Predicciones
        y_pred_test = xgb_model.predict(self.X_test_scaled)
        y_pred_proba = xgb_model.predict_proba(self.X_test_scaled)

        self.models['xgboost'] = xgb_model
        self.results['xgboost'] = {
            'train_accuracy': train_acc,
            'val_accuracy': val_acc,
            'test_accuracy': test_acc,
            'predictions_test': y_pred_test,
            'predictions_proba': y_pred_proba,
            'feature_importance': xgb_model.feature_importances_
        }

        print(f"XGBoost - Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}")
        return xgb_model

    def build_mlp_model(self, hidden_layers=[512, 256, 128], dropout_rate=0.3, activation='relu'):
        """
        Construye modelo MLP (Multi-Layer Perceptron)

        Args:
            hidden_layers (list): Tama√±os de capas ocultas
            dropout_rate (float): Tasa de dropout
            activation (str): Funci√≥n de activaci√≥n

        Returns:
            Sequential: Modelo MLP
        """
        model = Sequential()

        # Capa de entrada
        model.add(Dense(hidden_layers[0], activation=activation, input_shape=(self.X_train.shape[1],)))
        model.add(BatchNormalization())
        model.add(Dropout(dropout_rate))

        # Capas ocultas
        for units in hidden_layers[1:]:
            model.add(Dense(units, activation=activation))
            model.add(BatchNormalization())
            model.add(Dropout(dropout_rate))

        # Capa de salida
        model.add(Dense(self.n_classes, activation='softmax'))

        return model

    def build_cnn1d_model(self, input_shape, filters=[64, 128, 256], kernel_size=3, pool_size=2):
        """
        Construye modelo CNN 1D para secuencias temporales

        Args:
            input_shape (tuple): Forma de entrada
            filters (list): Filtros por capa
            kernel_size (int): Tama√±o del kernel
            pool_size (int): Tama√±o del pooling

        Returns:
            Sequential: Modelo CNN 1D
        """
        model = Sequential()

        # Primera capa convolucional
        model.add(Conv1D(filters[0], kernel_size, activation='relu', input_shape=input_shape))
        model.add(BatchNormalization())
        model.add(MaxPooling1D(pool_size))

        # Capas convolucionales adicionales
        for f in filters[1:]:
            model.add(Conv1D(f, kernel_size, activation='relu'))
            model.add(BatchNormalization())
            model.add(MaxPooling1D(pool_size))

        # Capas densas
        model.add(Flatten())
        model.add(Dense(512, activation='relu'))
        model.add(Dropout(0.5))
        model.add(Dense(256, activation='relu'))
        model.add(Dropout(0.3))
        model.add(Dense(self.n_classes, activation='softmax'))

        return model

    def build_lstm_model(self, input_shape, lstm_units=[128, 64], dropout_rate=0.3):
        """
        Construye modelo LSTM para secuencias temporales

        Args:
            input_shape (tuple): Forma de entrada
            lstm_units (list): Unidades LSTM por capa
            dropout_rate (float): Tasa de dropout

        Returns:
            Sequential: Modelo LSTM
        """
        model = Sequential()

        # Primera capa LSTM
        model.add(LSTM(lstm_units[0], return_sequences=len(lstm_units) > 1,
                       input_shape=input_shape))
        model.add(BatchNormalization())
        model.add(Dropout(dropout_rate))

        # Capas LSTM adicionales
        for i, units in enumerate(lstm_units[1:], 1):
            return_seq = i < len(lstm_units) - 1
            model.add(LSTM(units, return_sequences=return_seq))
            model.add(BatchNormalization())
            model.add(Dropout(dropout_rate))

        # Capas densas
        model.add(Dense(256, activation='relu'))
        model.add(Dropout(dropout_rate))
        model.add(Dense(self.n_classes, activation='softmax'))

        return model

    def build_hybrid_cnn_lstm_model(self, input_shape, cnn_filters=[64, 128],
                                    lstm_units=[128, 64], dropout_rate=0.3):
        """
        Construye modelo h√≠brido CNN + LSTM

        Args:
            input_shape (tuple): Forma de entrada
            cnn_filters (list): Filtros CNN
            lstm_units (list): Unidades LSTM
            dropout_rate (float): Tasa de dropout

        Returns:
            Sequential: Modelo h√≠brido
        """
        model = Sequential()

        # Capas CNN
        model.add(Conv1D(cnn_filters[0], 3, activation='relu', input_shape=input_shape))
        model.add(BatchNormalization())
        model.add(MaxPooling1D(2))

        for f in cnn_filters[1:]:
            model.add(Conv1D(f, 3, activation='relu'))
            model.add(BatchNormalization())
            model.add(MaxPooling1D(2))

        # Capas LSTM
        for i, units in enumerate(lstm_units):
            return_seq = i < len(lstm_units) - 1
            model.add(LSTM(units, return_sequences=return_seq))
            model.add(BatchNormalization())
            model.add(Dropout(dropout_rate))

        # Capas de salida
        model.add(Dense(256, activation='relu'))
        model.add(Dropout(dropout_rate))
        model.add(Dense(self.n_classes, activation='softmax'))

        return model

    def train_deep_model(self, model, model_name, epochs=100, batch_size=32,
                         learning_rate=0.001, patience=10, verbose=1):
        """
        Entrena un modelo de deep learning

        Args:
            model: Modelo de Keras
            model_name (str): Nombre del modelo
            epochs (int): N√∫mero de √©pocas
            batch_size (int): Tama√±o del batch
            learning_rate (float): Tasa de aprendizaje
            patience (int): Paciencia para early stopping
            verbose (int): Nivel de verbosidad

        Returns:
            tuple: (modelo_entrenado, historial)
        """
        print(f"Entrenando {model_name}...")

        # Compilar modelo
        optimizer = Adam(learning_rate=learning_rate)
        model.compile(
            optimizer=optimizer,
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )

        # Callbacks
        callbacks = [
            EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)
        ]

        # Entrenar
        history = model.fit(
            self.X_train_scaled, self.y_train_cat,
            validation_data=(self.X_val_scaled, self.y_val_cat),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=verbose
        )

        # Evaluar
        train_loss, train_acc = model.evaluate(self.X_train_scaled, self.y_train_cat, verbose=0)
        val_loss, val_acc = model.evaluate(self.X_val_scaled, self.y_val_cat, verbose=0)
        test_loss, test_acc = model.evaluate(self.X_test_scaled, self.y_test_cat, verbose=0)

        # Predicciones
        y_pred_test_proba = model.predict(self.X_test_scaled, verbose=0)
        y_pred_test = np.argmax(y_pred_test_proba, axis=1)

        # Guardar resultados
        self.models[model_name] = model
        self.history[model_name] = history
        self.results[model_name] = {
            'train_accuracy': train_acc,
            'val_accuracy': val_acc,
            'test_accuracy': test_acc,
            'train_loss': train_loss,
            'val_loss': val_loss,
            'test_loss': test_loss,
            'predictions_test': y_pred_test,
            'predictions_proba': y_pred_test_proba
        }

        print(f"{model_name} - Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}")
        return model, history

    def hyperparameter_tuning_rf(self, param_grid=None, cv=5):
        """
        Optimizaci√≥n de hiperpar√°metros para Random Forest - VERSI√ìN PARA DATASETS PEQUE√ëOS
        """
        if param_grid is None:
            param_grid = {
                'n_estimators': [50, 100],
                'max_depth': [10, 20],
                'min_samples_split': [2, 5]
            }

        print("Optimizando hiperpar√°metros Random Forest...")

        # AJUSTAR CV PARA DATASETS PEQUE√ëOS
        n_samples = len(self.X_train_scaled)
        min_samples_per_class = np.min(np.bincount(self.y_train))

        # Reducir CV si hay pocas muestras
        if min_samples_per_class < 5:
            cv = 2  # Solo 2 folds
            print(f"‚ö†Ô∏è Pocas muestras por clase ({min_samples_per_class}), usando cv={cv}")
        elif min_samples_per_class < 10:
            cv = 3  # 3 folds
            print(f"‚ö†Ô∏è Pocas muestras por clase ({min_samples_per_class}), usando cv={cv}")

        if n_samples < 10:
            # Muy pocas muestras - entrenar con par√°metros por defecto
            print("‚ö†Ô∏è Muy pocas muestras, saltando optimizaci√≥n de hiperpar√°metros...")
            best_rf = RandomForestClassifier(
                n_estimators=50,
                max_depth=10,
                random_state=self.random_state
            )
            best_rf.fit(self.X_train_scaled, self.y_train)

            test_acc = best_rf.score(self.X_test_scaled, self.y_test)
            y_pred_test = best_rf.predict(self.X_test_scaled)

            self.models['random_forest_tuned'] = best_rf
            self.results['random_forest_tuned'] = {
                'test_accuracy': test_acc,
                'predictions_test': y_pred_test,
                'best_params': {'n_estimators': 50, 'max_depth': 10},
                'cv_score': test_acc,  # Usar test como aproximaci√≥n
                'feature_importance': best_rf.feature_importances_
            }

            print(f"Modelo por defecto - Test: {test_acc:.4f}")
            return best_rf, {'n_estimators': 50, 'max_depth': 10}

        try:
            rf = RandomForestClassifier(random_state=self.random_state)
            grid_search = GridSearchCV(
                rf, param_grid, cv=cv, scoring='accuracy',
                n_jobs=1, verbose=1  # Cambiar n_jobs a 1 para evitar problemas
            )

            grid_search.fit(self.X_train_scaled, self.y_train)

            print(f"Mejores par√°metros: {grid_search.best_params_}")
            print(f"Mejor score CV: {grid_search.best_score_:.4f}")

            # Entrenar modelo con mejores par√°metros
            best_rf = grid_search.best_estimator_
            test_acc = best_rf.score(self.X_test_scaled, self.y_test)
            y_pred_test = best_rf.predict(self.X_test_scaled)

            self.models['random_forest_tuned'] = best_rf
            self.results['random_forest_tuned'] = {
                'test_accuracy': test_acc,
                'predictions_test': y_pred_test,
                'best_params': grid_search.best_params_,
                'cv_score': grid_search.best_score_,
                'feature_importance': best_rf.feature_importances_
            }

            return best_rf, grid_search.best_params_

        except Exception as e:
            print(f"‚ö†Ô∏è Error en optimizaci√≥n: {e}")
            print("Usando par√°metros por defecto...")

            # Fallback a par√°metros por defecto
            best_rf = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=self.random_state)
            best_rf.fit(self.X_train_scaled, self.y_train)

            test_acc = best_rf.score(self.X_test_scaled, self.y_test)
            y_pred_test = best_rf.predict(self.X_test_scaled)

            self.models['random_forest_tuned'] = best_rf
            self.results['random_forest_tuned'] = {
                'test_accuracy': test_acc,
                'predictions_test': y_pred_test,
                'best_params': {'n_estimators': 50, 'max_depth': 10},
                'cv_score': test_acc,
                'feature_importance': best_rf.feature_importances_
            }

            return best_rf, {'n_estimators': 50, 'max_depth': 10}

    def evaluate_all_models(self):
        """
        Eval√∫a todos los modelos entrenados

        Returns:
            pd.DataFrame: Resumen de resultados
        """
        print("\n" + "=" * 60)
        print("EVALUACI√ìN DE TODOS LOS MODELOS")
        print("=" * 60)

        results_summary = []

        for model_name, result in self.results.items():
            if 'test_accuracy' in result:
                results_summary.append({
                    'Model': model_name,
                    'Test_Accuracy': result['test_accuracy'],
                    'Train_Accuracy': result.get('train_accuracy', 'N/A'),
                    'Val_Accuracy': result.get('val_accuracy', 'N/A')
                })

        # Crear DataFrame y ordenar por accuracy
        df_results = pd.DataFrame(results_summary)
        df_results = df_results.sort_values('Test_Accuracy', ascending=False)

        print(df_results.to_string(index=False))
        # Encontrar mejor modelo
        if len(df_results) > 0:
            best_model_name = df_results.iloc[0]['Model']
            best_accuracy = df_results.iloc[0]['Test_Accuracy']
            print(f"\nMejor modelo: {best_model_name} (Accuracy: {best_accuracy:.4f})")

        return df_results

    def plot_training_history(self, model_names=None, figsize=(15, 10)):
        """
        Visualiza el historial de entrenamiento de modelos deep learning

        Args:
            model_names (list): Nombres de modelos a visualizar
            figsize (tuple): Tama√±o de la figura
        """
        if model_names is None:
            model_names = list(self.history.keys())

        n_models = len(model_names)
        if n_models == 0:
            print("No hay historial de entrenamiento disponible")
            return

        fig, axes = plt.subplots(n_models, 2, figsize=figsize)
        if n_models == 1:
            axes = axes.reshape(1, -1)

        for i, model_name in enumerate(model_names):
            if model_name not in self.history:
                continue

            history = self.history[model_name].history

            # Accuracy
            axes[i, 0].plot(history['accuracy'], label='Train Accuracy', color='blue')
            axes[i, 0].plot(history['val_accuracy'], label='Val Accuracy', color='red')
            axes[i, 0].set_title(f'{model_name} - Accuracy')
            axes[i, 0].set_xlabel('Epoch')
            axes[i, 0].set_ylabel('Accuracy')
            axes[i, 0].legend()
            axes[i, 0].grid(True, alpha=0.3)

            # Loss
            axes[i, 1].plot(history['loss'], label='Train Loss', color='blue')
            axes[i, 1].plot(history['val_loss'], label='Val Loss', color='red')
            axes[i, 1].set_title(f'{model_name} - Loss')
            axes[i, 1].set_xlabel('Epoch')
            axes[i, 1].set_ylabel('Loss')
            axes[i, 1].legend()
            axes[i, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def plot_confusion_matrices(self, model_names=None, figsize=(15, 10)):
        """
        Visualiza matrices de confusi√≥n para todos los modelos

        Args:
            model_names (list): Nombres de modelos a visualizar
            figsize (tuple): Tama√±o de la figura
        """
        if model_names is None:
            model_names = [name for name in self.results.keys()
                           if 'predictions_test' in self.results[name]]

        n_models = len(model_names)
        if n_models == 0:
            print("No hay predicciones disponibles")
            return

        # Calcular n√∫mero de filas y columnas
        n_cols = min(3, n_models)
        n_rows = (n_models + n_cols - 1) // n_cols

        fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)
        if n_models == 1:
            axes = [axes]
        elif n_rows == 1:
            axes = axes.reshape(1, -1)

        axes = axes.flatten()

        for i, model_name in enumerate(model_names):
            y_pred = self.results[model_name]['predictions_test']

            # Calcular matriz de confusi√≥n
            cm = confusion_matrix(self.y_test, y_pred)

            # Visualizar
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],
                        xticklabels=self.class_names, yticklabels=self.class_names)
            axes[i].set_title(f'{model_name}\nAccuracy: {self.results[model_name]["test_accuracy"]:.4f}')
            axes[i].set_xlabel('Predicci√≥n')
            axes[i].set_ylabel('Valor Real')

        # Ocultar axes vac√≠os
        for i in range(n_models, len(axes)):
            axes[i].set_visible(False)

        plt.tight_layout()
        plt.show()

    def generate_classification_reports(self, model_names=None):
        """
        Genera reportes de clasificaci√≥n detallados
        """
        if model_names is None:
            model_names = [name for name in self.results.keys()
                           if 'predictions_test' in self.results[name]]

        for model_name in model_names:
            print(f"\n{'=' * 60}")
            print(f"REPORTE DE CLASIFICACI√ìN - {model_name.upper()}")
            print(f"{'=' * 60}")

            y_pred = self.results[model_name]['predictions_test']

            # FIX: Obtener solo las clases presentes en el test set
            unique_test_classes = np.unique(self.y_test)
            unique_pred_classes = np.unique(y_pred)
            all_classes = np.unique(np.concatenate([unique_test_classes, unique_pred_classes]))

            # Crear nombres de clases solo para las clases presentes
            target_names = [f"Clase_{int(i)}" for i in all_classes]

            # Reporte detallado con labels espec√≠ficos
            try:
                report = classification_report(
                    self.y_test, y_pred,
                    labels=all_classes,  # ‚Üê A√ëADIR ESTA L√çNEA
                    target_names=target_names,  # ‚Üê USAR NOMBRES AJUSTADOS
                    output_dict=True,
                    zero_division=0  # ‚Üê A√ëADIR PARA EVITAR WARNINGS
                )
            except Exception as e:
                print(f"Error generando reporte para {model_name}: {e}")
                print("Usando reporte simplificado...")

                # Reporte simplificado sin target_names
                report = classification_report(
                    self.y_test, y_pred,
                    output_dict=True,
                    zero_division=0
                )
                target_names = [f"Clase_{i}" for i in range(len(np.unique(np.concatenate([self.y_test, y_pred]))))]

            # Mostrar m√©tricas por clase
            print(f"{'Clase':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}")
            print("-" * 60)

            # Solo mostrar clases que est√°n en el reporte
            for i, class_name in enumerate(target_names):
                class_key = str(all_classes[i]) if i < len(all_classes) else str(i)
                if class_key in report:
                    metrics = report[class_key]
                    print(f"{class_name:<15} {metrics['precision']:<10.4f} {metrics['recall']:<10.4f} "
                          f"{metrics['f1-score']:<10.4f} {int(metrics['support']):<10}")

            # M√©tricas globales
            if 'accuracy' in report:
                print("-" * 60)
                print(
                    f"{'Accuracy':<15} {'':<10} {'':<10} {report['accuracy']:<10.4f} {int(report['macro avg']['support']):<10}")
                print(
                    f"{'Macro Avg':<15} {report['macro avg']['precision']:<10.4f} {report['macro avg']['recall']:<10.4f} "
                    f"{report['macro avg']['f1-score']:<10.4f} {int(report['macro avg']['support']):<10}")
                print(
                    f"{'Weighted Avg':<15} {report['weighted avg']['precision']:<10.4f} {report['weighted avg']['recall']:<10.4f} "
                    f"{report['weighted avg']['f1-score']:<10.4f} {int(report['weighted avg']['support']):<10}")

def compare_model_performance(results_dict, figsize=(12, 6)):
    """
    Compara el rendimiento de diferentes modelos

    Args:
        results_dict (dict): Diccionario con resultados de modelos
        figsize (tuple): Tama√±o de la figura

    Returns:
        pd.DataFrame: DataFrame con comparaci√≥n
    """
    metrics_data = []

    for model_name, results in results_dict.items():
        if 'test_accuracy' in results:
            metrics_data.append({
                'Model': model_name,
                'Accuracy': results['test_accuracy'],
                'Type': 'Deep Learning' if model_name in ['mlp', 'cnn1d', 'lstm', 'hybrid'] else 'Traditional'
            })

    df = pd.DataFrame(metrics_data)

    if len(df) == 0:
        print("No hay datos para comparar")
        return df

    # Visualizaci√≥n
    plt.figure(figsize=figsize)

    # Gr√°fico de barras
    plt.subplot(1, 2, 1)
    colors = ['skyblue' if t == 'Traditional' else 'orange' for t in df['Type']]
    bars = plt.bar(range(len(df)), df['Accuracy'], color=colors)
    plt.xticks(range(len(df)), df['Model'], rotation=45, ha='right')
    plt.ylabel('Test Accuracy')
    plt.title('Comparaci√≥n de Accuracy por Modelo')
    plt.grid(True, alpha=0.3)

    # A√±adir valores en las barras
    for bar, acc in zip(bars, df['Accuracy']):
        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.005,
                 f'{acc:.3f}', ha='center', va='bottom')

    # Boxplot por tipo
    plt.subplot(1, 2, 2)
    traditional_acc = df[df['Type'] == 'Traditional']['Accuracy'].values
    dl_acc = df[df['Type'] == 'Deep Learning']['Accuracy'].values

    data_to_plot = []
    labels = []
    if len(traditional_acc) > 0:
        data_to_plot.append(traditional_acc)
        labels.append('Traditional')
    if len(dl_acc) > 0:
        data_to_plot.append(dl_acc)
        labels.append('Deep Learning')

    if data_to_plot:
        plt.boxplot(data_to_plot, labels=labels)
        plt.ylabel('Test Accuracy')
        plt.title('Distribuci√≥n de Accuracy por Tipo')
        plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    return df


def analyze_feature_importance(models_dict, feature_names=None, figsize=(15, 10)):
    """
    Analiza la importancia de caracter√≠sticas en modelos que la soportan

    Args:
        models_dict (dict): Diccionario con resultados de modelos
        feature_names (list): Nombres de las caracter√≠sticas
        figsize (tuple): Tama√±o de la figura
    """
    models_with_importance = ['random_forest', 'xgboost', 'random_forest_tuned']

    # Filtrar modelos que tienen importancia de caracter√≠sticas
    available_models = [m for m in models_with_importance
                        if m in models_dict and 'feature_importance' in models_dict[m]]

    if not available_models:
        print("No hay modelos con importancia de caracter√≠sticas disponible")
        return

    plt.figure(figsize=figsize)

    plot_idx = 1
    for model_name in available_models:
        importance = models_dict[model_name]['feature_importance']

        # Obtener √≠ndices de las caracter√≠sticas m√°s importantes
        top_indices = np.argsort(importance)[-20:]  # Top 20
        top_importance = importance[top_indices]

        if feature_names is not None:
            top_names = [feature_names[i] for i in top_indices]
        else:
            top_names = [f'Feature_{i}' for i in top_indices]

        plt.subplot(len(available_models), 1, plot_idx)
        plt.barh(range(len(top_importance)), top_importance, color='skyblue')
        plt.yticks(range(len(top_importance)), [name[:50] + '...' if len(name) > 50 else name
                                                for name in top_names])
        plt.xlabel('Importancia')
        plt.title(f'Top 20 Caracter√≠sticas M√°s Importantes - {model_name}')
        plt.grid(True, alpha=0.3)

        plot_idx += 1

    plt.tight_layout()
    plt.show()


def cross_validation_analysis(X, y, models_dict=None, cv_folds=5, random_state=42):
    """
    Realiza an√°lisis de validaci√≥n cruzada

    Args:
        X: Caracter√≠sticas
        y: Etiquetas
        models_dict (dict): Diccionario de modelos (opcional)
        cv_folds (int): N√∫mero de folds
        random_state (int): Semilla aleatoria

    Returns:
        dict: Resultados de validaci√≥n cruzada
    """
    print("Realizando an√°lisis de validaci√≥n cruzada...")

    cv_results = {}

    # Solo modelos tradicionales (sklearn)
    sklearn_models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state),
        'SVM': SVC(C=1.0, random_state=random_state),
        'XGBoost': xgb.XGBClassifier(n_estimators=100, random_state=random_state, eval_metric='mlogloss')
    }

    for model_name, model in sklearn_models.items():
        print(f"Evaluando {model_name}...")
        scores = cross_val_score(model, X, y, cv=cv_folds, scoring='accuracy')

        cv_results[model_name] = {
            'scores': scores,
            'mean': scores.mean(),
            'std': scores.std()
        }

        print(f"  {model_name}: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")

    # Visualizaci√≥n
    plt.figure(figsize=(10, 6))

    models = list(cv_results.keys())
    means = [cv_results[m]['mean'] for m in models]
    stds = [cv_results[m]['std'] for m in models]

    plt.bar(models, means, yerr=stds, capsize=5, alpha=0.7, color='skyblue')
    plt.ylabel('Accuracy')
    plt.title(f'Validaci√≥n Cruzada ({cv_folds}-fold)')
    plt.grid(True, alpha=0.3)

    # A√±adir valores
    for i, (mean, std) in enumerate(zip(means, stds)):
        plt.text(i, mean + std + 0.01, f'{mean:.3f}¬±{std:.3f}',
                 ha='center', va='bottom')

    plt.tight_layout()
    plt.show()

    return cv_results


def save_model_results(model_pipeline, filepath):
    """
    Guarda resultados del pipeline de modelado

    Args:
        model_pipeline: Pipeline de modelos
        filepath (str): Ruta del archivo
    """
    import pickle

    # Preparar datos para guardado (sin modelos de Keras que son grandes)
    data_to_save = {
        'results': model_pipeline.results,
        'label_encoder': model_pipeline.label_encoder,
        'scaler': model_pipeline.scaler,
        'n_classes': model_pipeline.n_classes,
        'class_names': model_pipeline.class_names,
        'random_state': model_pipeline.random_state
    }

    with open(filepath, 'wb') as f:
        pickle.dump(data_to_save, f)

    print(f"Resultados guardados en: {filepath}")


def create_model_comparison_report(model_pipeline):
    """
    Crea un reporte comparativo detallado de todos los modelos

    Args:
        model_pipeline: Pipeline de modelos
    """
    print("\n" + "=" * 80)
    print("REPORTE COMPARATIVO DE MODELOS")
    print("=" * 80)

    # Tabla resumen
    results_data = []
    for model_name, results in model_pipeline.results.items():
        if 'test_accuracy' in results:
            results_data.append({
                'Modelo': model_name,
                'Accuracy_Test': results['test_accuracy'],
                'Accuracy_Train': results.get('train_accuracy', 'N/A'),
                'Accuracy_Val': results.get('val_accuracy', 'N/A'),
                'Tipo': 'Deep Learning' if model_name in ['mlp', 'cnn1d', 'lstm', 'hybrid'] else 'Tradicional'
            })

    if not results_data:
        print("No hay resultados para mostrar")
        return

    df_results = pd.DataFrame(results_data)
    df_results = df_results.sort_values('Accuracy_Test', ascending=False)

    print("\nTABLA COMPARATIVA:")
    print(df_results.to_string(index=False, float_format='%.4f'))

    # An√°lisis por tipo
    print(f"\nAN√ÅLISIS POR TIPO DE MODELO:")
    for tipo in df_results['Tipo'].unique():
        subset = df_results[df_results['Tipo'] == tipo]
        print(f"\n{tipo}:")
        print(f"  Mejor accuracy: {subset['Accuracy_Test'].max():.4f}")
        print(f"  Promedio: {subset['Accuracy_Test'].mean():.4f}")
        print(f"  Desv. est√°ndar: {subset['Accuracy_Test'].std():.4f}")

    # Recomendaciones
    print(f"\nRECOMENDACIONES:")
    best_model = df_results.iloc[0]
    print(f"- Mejor modelo general: {best_model['Modelo']} ({best_model['Accuracy_Test']:.4f})")

    if best_model['Tipo'] == 'Tradicional':
        print("- Los modelos tradicionales funcionan bien para este problema")
        print("- Considera usar el modelo con mejor interpretabilidad")
    else:
        print("- Los modelos de deep learning superan a los tradicionales")
        print("- Considera el costo computacional vs. mejora en rendimiento")

    # Verificar overfitting
    for _, row in df_results.iterrows():
        if row['Accuracy_Train'] != 'N/A':
            train_acc = float(row['Accuracy_Train'])
            test_acc = float(row['Accuracy_Test'])
            if train_acc - test_acc > 0.1:
                print(f"- ADVERTENCIA: {row['Modelo']} muestra signos de overfitting")


def demo_complete_modeling():
    """
    Demostraci√≥n completa del pipeline de modelado
    """
    print("=== DEMOSTRACI√ìN COMPLETA DE MODELADO ===")

    # Generar datos sint√©ticos
    np.random.seed(42)
    n_samples = 1000
    n_features = 100
    n_classes = 4

    # Crear caracter√≠sticas sint√©ticas con patrones
    X = np.random.randn(n_samples, n_features)

    # Crear etiquetas con patrones espec√≠ficos
    y = np.zeros(n_samples)
    for i in range(n_samples):
        # Diferentes patrones para diferentes clases
        if np.mean(X[i, :20]) > 0:  # Clase basada en primeras 20 features
            if np.std(X[i, 20:40]) > 1:
                y[i] = 0
            else:
                y[i] = 1
        else:
            if np.mean(X[i, 40:60]) > 0:
                y[i] = 2
            else:
                y[i] = 3

    print(f"Datos sint√©ticos: {X.shape}")
    print(f"Distribuci√≥n de clases: {np.bincount(y.astype(int))}")

    # Crear pipeline de modelado
    model_pipeline = ActivityRecognitionModels(random_state=42)

    # Preparar datos
    X_train_scaled, X_val_scaled, X_test_scaled = model_pipeline.prepare_data(X, y)

    # Entrenar modelos tradicionales
    print("\n--- ENTRENANDO MODELOS TRADICIONALES ---")
    model_pipeline.train_random_forest(n_estimators=100, max_depth=10)
    model_pipeline.train_svm(C=1.0, kernel='rbf')
    model_pipeline.train_xgboost(n_estimators=100, max_depth=6)

    # Entrenar modelos de deep learning
    print("\n--- ENTRENANDO MODELOS DEEP LEARNING ---")

    # MLP
    mlp_model = model_pipeline.build_mlp_model(hidden_layers=[256, 128, 64])
    model_pipeline.train_deep_model(mlp_model, 'mlp', epochs=50, batch_size=32, verbose=0)

    # Optimizaci√≥n de hiperpar√°metros
    print("\n--- OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS ---")
    best_rf, best_params = model_pipeline.hyperparameter_tuning_rf()

    # Evaluaci√≥n completa
    print("\n--- EVALUACI√ìN COMPLETA ---")
    results_df = model_pipeline.evaluate_all_models()

    # Visualizaciones
    print("\n--- VISUALIZACIONES ---")
    model_pipeline.plot_training_history()
    model_pipeline.plot_confusion_matrices()
    model_pipeline.generate_classification_reports()

    # An√°lisis comparativo
    performance_df = compare_model_performance(model_pipeline.results)

    # An√°lisis de validaci√≥n cruzada
    cv_results = cross_validation_analysis(X_train_scaled, model_pipeline.y_train)

    print("\n=== RESUMEN FINAL ===")
    if len(results_df) > 0:
        best_model = results_df.iloc[0]
        print(f"Mejor modelo: {best_model['Model']}")
        print(f"Accuracy de prueba: {best_model['Test_Accuracy']:.4f}")

    # Crear reporte final
    create_model_comparison_report(model_pipeline)

    return model_pipeline


if __name__ == "__main__":
    # Ejecutar demostraci√≥n completa
    model_pipeline = demo_complete_modeling()

    print("\nüéâ Modelado completado exitosamente!")
    print(f"üìä {len(model_pipeline.results)} modelos entrenados")
    if model_pipeline.results:
        best_acc = max(r['test_accuracy'] for r in model_pipeline.results.values() if 'test_accuracy' in r)
        print(f"üèÜ Mejor accuracy: {best_acc:.4f}")